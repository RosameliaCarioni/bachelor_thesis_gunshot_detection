{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from methods_audio import data_handling\n",
    "from methods_audio import data_augmentation\n",
    "from methods_audio import denoising \n",
    "from methods_audio import model_performance_training\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get data (file names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_handling.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read data (transforming file names into waves) <br>\n",
    "Additionally, the mean is removed and the data is normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(data_handling.read_in_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get input for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 16:15:06.984741: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "samples, labels = data_handling.extract_samples_labels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_size = 0.30\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(samples, labels, test_size= validation_set_size, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transform data to db mel-spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_transformation = 'db_mel_spectrogram'\n",
    "x_train = data_handling.transform_data(x_train, type_transformation)\n",
    "x_valid = data_handling.transform_data(x_valid, type_transformation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build model with hyperparameter tunning \n",
    "https://keras.io/guides/keras_tuner/getting_started/ <br>\n",
    "https://www.youtube.com/watch?v=6Nf1x7qThR8&ab_channel=GregHogg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input = (624, 128, 1)\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer \n",
    "    #matching samples.shape\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters= hp.Int(\"conv_filters_0\", min_value=8, max_value=128, step=16), \n",
    "            activation= hp.Choice(\"conv_activation_0\", [\"relu\", \"tanh\"]),\n",
    "            kernel_size = (3,3), \n",
    "            input_shape=input\n",
    "        )\n",
    "    ) \n",
    "    model.add(MaxPool2D(pool_size= (2,2)))\n",
    "\n",
    "    # Tune the number of Conv layers \n",
    "    for i in range(hp.Int(\"num_conv_layers\", 1, 4)):\n",
    "        model.add(\n",
    "            Sequential([\n",
    "                layers.Conv2D(\n",
    "                    filters=hp.Int(f\"conv_filters_{i}\", min_value=8, max_value=128, step=16),\n",
    "                    activation=hp.Choice(f\"conv_activation_{i}\", [\"relu\", \"tanh\"]),\n",
    "                    kernel_size=(4,3),\n",
    "                ), \n",
    "                layers.MaxPool2D(pool_size=(2,2)),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Tune the number of Dense layers and Tune whether to use dropout layer\n",
    "    for i in range(hp.Int(\"num_dense_layers\", 1, 6)):\n",
    "            model.add(\n",
    "                Sequential([\n",
    "                    layers.Dense(\n",
    "                        # Tune number of units separately.\n",
    "                        units=hp.Int(f\"dense_units_{i}\", min_value=50, max_value=600, step=50),\n",
    "                        activation=hp.Choice(f\"dense_activation_{i}\", [\"relu\", \"tanh\"]),\n",
    "                    ), \n",
    "                    layers.Dropout(\n",
    "                        rate=hp.Float(f\"dense_dropout_{i}\", min_value = 0, max_value = 1)\n",
    "                    )\n",
    "                ]) \n",
    "            )\n",
    "\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "        units=1, #because we have 2 classes \n",
    "        activation=hp.Choice(\"activatio_last_layer\", [\"softmax\", \"sigmoid\"]), \n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the optimizer learning rate as a hyperparameter.\n",
    "    # sampling=\"log\", the step is multiplied between samples.\n",
    "    lr= hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=lr), \n",
    "        loss=\"BinaryCrossentropy\", \n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2872da940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize tuner by specifying different arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective= \"val_accuracy\", # we want maximize accuracy \n",
    "    max_trials= 100,\n",
    "    overwrite=True,\n",
    "    directory=\"param_optimization\", \n",
    "    project_name=\"db_mel_spectrogram\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', patience=10) \n",
    "# patience refers to number of epochs: if the val loss is not improving fter 10 ephocs, we stop it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During the search, the model is called with different hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "conv_filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 8, 'max_value': 128, 'step': 16, 'sampling': 'linear'}\n",
      "conv_activation_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "num_conv_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "num_dense_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 6, 'step': 1, 'sampling': 'linear'}\n",
      "dense_units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 50, 'max_value': 600, 'step': 50, 'sampling': 'linear'}\n",
      "dense_activation_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "dense_dropout_0 (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 1.0, 'step': None, 'sampling': 'linear'}\n",
      "activatio_last_layer (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.1, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()\n",
    "# Default search space size: number of hyper parameters that we are tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "40                |40                |conv_filters_0\n",
      "relu              |relu              |conv_activation_0\n",
      "3                 |3                 |num_conv_layers\n",
      "1                 |1                 |num_dense_layers\n",
      "350               |350               |dense_units_0\n",
      "tanh              |tanh              |dense_activation_0\n",
      "0.02457           |0.02457           |dense_dropout_0\n",
      "softmax           |softmax           |activatio_last_layer\n",
      "0.00055746        |0.00055746        |learning_rate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 6/78 [=>............................] - ETA: 18s - loss: 0.5985 - accuracy: 0.4583WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0879s vs `on_train_batch_end` time: 0.1527s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0879s vs `on_train_batch_end` time: 0.1527s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 23s 275ms/step - loss: 0.5134 - accuracy: 0.4958 - val_loss: 0.5057 - val_accuracy: 0.5042\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 21s 272ms/step - loss: 0.4725 - accuracy: 0.4958 - val_loss: 0.4938 - val_accuracy: 0.5042\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 22s 286ms/step - loss: 0.4615 - accuracy: 0.4958 - val_loss: 0.4795 - val_accuracy: 0.5042\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 21s 267ms/step - loss: 0.4560 - accuracy: 0.4958 - val_loss: 0.4669 - val_accuracy: 0.5042\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 22s 280ms/step - loss: 0.4574 - accuracy: 0.4958 - val_loss: 0.5267 - val_accuracy: 0.5042\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 21s 275ms/step - loss: 0.4516 - accuracy: 0.4958 - val_loss: 0.4647 - val_accuracy: 0.5042\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 21s 263ms/step - loss: 0.4385 - accuracy: 0.4958 - val_loss: 0.4548 - val_accuracy: 0.5042\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 0.4351 - accuracy: 0.4958 - val_loss: 0.4564 - val_accuracy: 0.5042\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 0.4356 - accuracy: 0.4958 - val_loss: 0.4687 - val_accuracy: 0.5042\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 21s 272ms/step - loss: 0.4322 - accuracy: 0.4958 - val_loss: 0.5291 - val_accuracy: 0.5042\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 0.4407 - accuracy: 0.4958 - val_loss: 0.4415 - val_accuracy: 0.5042\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.4324 - accuracy: 0.4958 - val_loss: 0.4567 - val_accuracy: 0.5042\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.4356 - accuracy: 0.4958 - val_loss: 0.4528 - val_accuracy: 0.5042\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 0.4279 - accuracy: 0.4958 - val_loss: 0.4942 - val_accuracy: 0.5042\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 20s 262ms/step - loss: 0.4247 - accuracy: 0.4958 - val_loss: 0.4498 - val_accuracy: 0.5042\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 0.4225 - accuracy: 0.4958 - val_loss: 0.5841 - val_accuracy: 0.5042\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.4156 - accuracy: 0.4958 - val_loss: 0.4316 - val_accuracy: 0.5042\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.4106 - accuracy: 0.4958 - val_loss: 0.4187 - val_accuracy: 0.5042\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 0.4094 - accuracy: 0.4958 - val_loss: 0.4343 - val_accuracy: 0.5042\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.4164 - accuracy: 0.4958 - val_loss: 0.4548 - val_accuracy: 0.5042\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 20s 254ms/step - loss: 0.4055 - accuracy: 0.4958 - val_loss: 0.4765 - val_accuracy: 0.5042\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 20s 260ms/step - loss: 0.4250 - accuracy: 0.4958 - val_loss: 0.4737 - val_accuracy: 0.5042\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.4116 - accuracy: 0.4958 - val_loss: 0.4189 - val_accuracy: 0.5042\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.4089 - accuracy: 0.4958 - val_loss: 0.4086 - val_accuracy: 0.5042\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 20s 253ms/step - loss: 0.3955 - accuracy: 0.4958 - val_loss: 0.4862 - val_accuracy: 0.5042\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 20s 251ms/step - loss: 0.4000 - accuracy: 0.4958 - val_loss: 0.4518 - val_accuracy: 0.5042\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 20s 251ms/step - loss: 0.4015 - accuracy: 0.4958 - val_loss: 0.4026 - val_accuracy: 0.5042\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 20s 251ms/step - loss: 0.4026 - accuracy: 0.4958 - val_loss: 0.4067 - val_accuracy: 0.5042\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 20s 252ms/step - loss: 0.3934 - accuracy: 0.4958 - val_loss: 0.5147 - val_accuracy: 0.5042\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.3868 - accuracy: 0.4958 - val_loss: 0.4061 - val_accuracy: 0.5042\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 20s 259ms/step - loss: 0.3899 - accuracy: 0.4958 - val_loss: 0.4049 - val_accuracy: 0.5042\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 0.3842 - accuracy: 0.4958 - val_loss: 0.4725 - val_accuracy: 0.5042\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.3762 - accuracy: 0.4958 - val_loss: 0.4255 - val_accuracy: 0.5042\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 0.3829 - accuracy: 0.4958 - val_loss: 0.3836 - val_accuracy: 0.5042\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 20s 255ms/step - loss: 0.3851 - accuracy: 0.4958 - val_loss: 0.4485 - val_accuracy: 0.5042\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 0.3742 - accuracy: 0.4958 - val_loss: 0.3875 - val_accuracy: 0.5042\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 21s 267ms/step - loss: 0.3786 - accuracy: 0.4958 - val_loss: 0.4216 - val_accuracy: 0.5042\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 20s 262ms/step - loss: 0.3832 - accuracy: 0.4958 - val_loss: 0.4222 - val_accuracy: 0.5042\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.3817 - accuracy: 0.4958 - val_loss: 0.4116 - val_accuracy: 0.5042\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 20s 258ms/step - loss: 0.3646 - accuracy: 0.4958 - val_loss: 0.3903 - val_accuracy: 0.5042\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 20s 257ms/step - loss: 0.3779 - accuracy: 0.4958 - val_loss: 0.3821 - val_accuracy: 0.5042\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 20s 256ms/step - loss: 0.3567 - accuracy: 0.4958 - val_loss: 0.3985 - val_accuracy: 0.5042\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 21s 264ms/step - loss: 0.3670 - accuracy: 0.4958 - val_loss: 0.3698 - val_accuracy: 0.5042\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 19s 244ms/step - loss: 0.3623 - accuracy: 0.4958 - val_loss: 0.3650 - val_accuracy: 0.5042\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 19s 238ms/step - loss: 0.3621 - accuracy: 0.4958 - val_loss: 0.3988 - val_accuracy: 0.5042\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 18s 236ms/step - loss: 0.3528 - accuracy: 0.4958 - val_loss: 0.4589 - val_accuracy: 0.5042\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 18s 237ms/step - loss: 0.3559 - accuracy: 0.4958 - val_loss: 0.7342 - val_accuracy: 0.5042\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 18s 236ms/step - loss: 0.3527 - accuracy: 0.4958 - val_loss: 0.3746 - val_accuracy: 0.5042\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 18s 237ms/step - loss: 0.3436 - accuracy: 0.4958 - val_loss: 0.3541 - val_accuracy: 0.5042\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 19s 240ms/step - loss: 0.3448 - accuracy: 0.4958 - val_loss: 0.3569 - val_accuracy: 0.5042\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 19s 239ms/step - loss: 0.3497 - accuracy: 0.4958 - val_loss: 0.5736 - val_accuracy: 0.5042\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.4958"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "start_time = time.time()\n",
    "\n",
    "tuner.search(np.stack(x_train), np.stack(y_train), epochs= epochs, validation_data=(np.stack(x_valid), np.stack(y_valid)), callbacks=[stop_early]) #similar to fit \n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"The search took {elapsed_time:.2f} seconds to finish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After all of that we don't have a model yet but rather a set of hyper parameters. Let's query the results and create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'data/models/db_mel_spectrogram'\n",
    "model.save(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach_thesis_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
