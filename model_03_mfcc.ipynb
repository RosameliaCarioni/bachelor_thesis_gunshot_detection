{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras_tuner\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from methods_audio import data_handling\n",
    "from methods_audio import data_augmentation\n",
    "from methods_audio import denoising \n",
    "from methods_audio import model_performance_training\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get data (file names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_handling.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read data (transforming file names into waves) <br>\n",
    "Additionally, the mean is removed and the data is normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(data_handling.read_in_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get input for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = data_handling.extract_samples_labels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_size = 0.30\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(samples, labels, test_size= validation_set_size, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transform data to spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_transformation = 'mfcc'\n",
    "x_train = data_handling.transform_data(x_train, type_transformation)\n",
    "x_valid = data_handling.transform_data(x_valid, type_transformation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build model with hyperparameter tunning \n",
    "https://keras.io/guides/keras_tuner/getting_started/ <br>\n",
    "https://www.youtube.com/watch?v=6Nf1x7qThR8&ab_channel=GregHogg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input = (624, 129,1)\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Add input layer \n",
    "    #matching samples.shape\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters= hp.Int(\"conv_filters_0\", min_value=16, max_value=128, step=16), \n",
    "            activation= hp.Choice(\"conv_activation_0\", [\"relu\", \"tanh\"]),\n",
    "            kernel_size = (3,3), \n",
    "            input_shape=input\n",
    "        )\n",
    "    ) \n",
    "    model.add(MaxPool2D(pool_size= (2,2)))\n",
    "\n",
    "    # Tune the number of Conv layers \n",
    "    for i in range(hp.Int(\"num_conv_layers\", 1, 3)):\n",
    "        model.add(\n",
    "            Sequential([\n",
    "                layers.Conv2D(\n",
    "                    filters=hp.Int(f\"conv_filters_{i}\", min_value=16, max_value=128, step=16),\n",
    "                    activation=hp.Choice(f\"conv_activation_{i}\", [\"relu\", \"tanh\"]),\n",
    "                    kernel_size=(3,3),\n",
    "                ), \n",
    "                layers.MaxPool2D(pool_size=(2,2)),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Tune the number of Dense layers and Tune whether to use dropout layer\n",
    "    for i in range(hp.Int(\"num_dense_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                Sequential([\n",
    "                    layers.Dense(\n",
    "                        # Tune number of units separately.\n",
    "                        units=hp.Int(f\"dense_units_{i}\", min_value=50, max_value=600, step=50),\n",
    "                        activation=hp.Choice(f\"dense_activation_{i}\", [\"relu\", \"tanh\"]),\n",
    "                    ), \n",
    "                    layers.Dropout(\n",
    "                        rate=hp.Float(f\"dense_dropout_{i}\", min_value = 0, max_value = 1)\n",
    "                    )\n",
    "                ]) \n",
    "            )\n",
    "\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "        units=1, #because we have 2 classes \n",
    "        activation=hp.Choice(\"activatio_last_layer\", [\"softmax\", \"sigmoid\"]), \n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the optimizer learning rate as a hyperparameter.\n",
    "    # sampling=\"log\", the step is multiplied between samples.\n",
    "    lr= hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=lr), \n",
    "        loss=\"BinaryCrossentropy\", \n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize tuner by specifying different arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model,\n",
    "    objective= \"val_accuracy\", # we want maximize accuracy \n",
    "    overwrite=True,\n",
    "    directory=\"param_optimization2\",\n",
    "    project_name=\"first_try\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', patience=5) \n",
    "# patience refers to number of epochs: if the val loss is not improving fter 5 ephocs, we stop it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During the search, the model is called with different hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()\n",
    "# Default search space size: number of hyper parameters that we are tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "tuner.search(np.stack(x_train), np.stack(y_train), epochs= epochs, validation_data=(np.stack(x_valid), np.stack(y_valid)), callbacks=[stop_early]) #similar to fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After all of that we don't have a model yet but rather a set of hyper parameters. Let's query the results and create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps) #saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'data/models/02'\n",
    "model.save(location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluate model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on a validation set\n",
    "loss, accuracy = model.evaluate(np.stack(x_valid), np.stack(y_valid))\n",
    "\n",
    "# print the evaluation results\n",
    "print(f'Validation loss: {loss:.4f}')\n",
    "print(f'Validation accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.stack(x_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = [1 if prediction > 0.5 else 0 for prediction in predictions]\n",
    "accuracy = accuracy_score(np.stack(y_valid), y_pred)\n",
    "accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(np.stack(y_valid), y_pred)\n",
    "recall = recall_score(np.stack(y_valid), y_pred)\n",
    "f1 = f1_score(np.stack(y_valid), y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach_thesis_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
